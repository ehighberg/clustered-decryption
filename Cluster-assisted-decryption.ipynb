{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from math import gcd\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:90% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:90% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "\n",
    "cipher_params = {'min_length': 500,\n",
    "                 'max_length': 1000,\n",
    "                 'messages_per_source': 10,\n",
    "                 'key_length': 5}\n",
    "\n",
    "decipher_params = {'n_top_trigrams_per_message': 100,\n",
    "                   'n_grams': 3,\n",
    "                   'kasiski_n': 4,\n",
    "                   'min_repeats': 6,\n",
    "                   'max_period': 100,\n",
    "                   'monte_carlo_samples_key_len': 10,\n",
    "                   'monte_carlo_num_trials': 1000,\n",
    "                   'monte_carlo_sample_fraction': 6}\n",
    "\n",
    "vector_params = {'top_N': 10,\n",
    "                 'min_matches': 3}\n",
    "\n",
    "agg_params = {'affinity': 'precomputed',\n",
    "              'linkage': 'single'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in and clean raw text\n",
    "### Read in all the source documents\n",
    "### Replace digits, newlines, and other escapes with spaces\n",
    "### Uppercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify_document(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        doc_as_string = f.read()\n",
    "        f.close()\n",
    "        return doc_as_string\n",
    "\n",
    "def clean_string(string):\n",
    "    return re.sub('[^a-zA-Z]', ' ', string).upper()\n",
    "\n",
    "def clean_docs(folderpath):\n",
    "    doc_strings = [stringify_document(os.path.join(folderpath, name)) for name in os.listdir(folderpath)]\n",
    "    clean_doc_strings = [clean_string(string) for string in doc_strings]\n",
    "    return clean_doc_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ciphertexts\n",
    "### Tokenize plaintext\n",
    "###  Extract 10 plaintexts from each source, of variable length, for each document\n",
    "### Encipher plaintexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plaintext_fragment(word_tokens):\n",
    "    length = np.random.randint(cipher_params['min_length'], cipher_params['max_length'])\n",
    "    start_point = np.random.randint(0, len(word_tokens) - length)\n",
    "    cand_plaintext = ''.join(word_tokens[start_point: start_point + length])\n",
    "    while len(cand_plaintext) % cipher_params['key_length'] != 0:\n",
    "        cand_plaintext = ''.join([cand_plaintext, 'A'])\n",
    "    return cand_plaintext\n",
    "\n",
    "\n",
    "def encrypt_message(key, message):\n",
    "    return translate_message(key, message, 'encrypt')\n",
    "\n",
    "def decrypt_message(key, message):\n",
    "    return translate_message(key, message, 'decrypt')\n",
    "\n",
    "def translate_message(key, message, mode):\n",
    "    \n",
    "    key_indices = [letters.index(key_letter) for key_letter in key]\n",
    "    \n",
    "    def cipher_letter(letter, key_index):\n",
    "        if mode == 'encrypt':\n",
    "            offset = key_index\n",
    "        elif mode == 'decrypt':\n",
    "            offset = -key_index\n",
    "        \n",
    "        return letters[(letters.index(letter) + offset) % len(letters)]\n",
    "    \n",
    "    return ''.join([cipher_letter(message[i], key_indices[i % len(key)]) for i in range(len(message))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_token_document(filepath):\n",
    "    print('filepath', filepath)\n",
    "    return word_tokenize(clean_string(stringify_document(filepath)))\n",
    "\n",
    "def divide_doc_into_messages(words, key):\n",
    "    return [encrypt_message(key, get_plaintext_fragment(words))\n",
    "            for _ in range(cipher_params['messages_per_source'])]\n",
    "\n",
    "def generate_key(length):\n",
    "    return ''.join([letters[np.random.randint(26)] for _ in range(length)])\n",
    "\n",
    "\n",
    "def gen_ciphered_texts(source_folder):\n",
    "    num_docs = len(os.listdir(source_folder))\n",
    "    ciphertexts = {}\n",
    "    \n",
    "    ciphertexts['document_keys'] = [generate_key(cipher_params['key_length'])\n",
    "                                    for _ in range(num_docs)]\n",
    "    \n",
    "    doc_filepaths = [os.path.join(source_folder, name) for name in os.listdir(source_folder)]\n",
    "    \n",
    "    ciphertexts['ciphertexts'] = [divide_doc_into_messages(clean_and_token_document(doc_filepaths[i]),\n",
    "                                                           ciphertexts['document_keys'][i])\n",
    "                                  for i in range(num_docs)]\n",
    "    \n",
    "    return ciphertexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize ciphertexts\n",
    "### Shuffle, then tag ciphertexts with unique IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_texts(texts):\n",
    "    texts = np.array(texts).ravel()\n",
    "    np.random.shuffle(texts)\n",
    "    return texts\n",
    "\n",
    "def tag_shuffled_texts(shuffled_texts):\n",
    "    return {i: {'text': text} for i, text in enumerate(shuffled_texts)}\n",
    "\n",
    "def shuffle_and_tag(texts):\n",
    "    return tag_shuffled_texts(shuffle_texts(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ciphertexts(folderpath):\n",
    "    document_ciphertexts = gen_ciphered_texts(folderpath)\n",
    "    message_ciphertexts = shuffle_and_tag(document_ciphertexts['ciphertexts'])\n",
    "    return message_ciphertexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath data/tng_val/294-0.txt\n",
      "filepath data/tng_val/108-0.txt\n",
      "filepath data/tng_val/2350-0.txt\n",
      "filepath data/tng_val/pg139.txt\n",
      "filepath data/tng_val/3289-0.txt\n",
      "filepath data/tng_val/pg126.txt\n",
      "filepath data/tng_val/2852-0.txt\n",
      "filepath data/tng_val/pg537.txt\n",
      "filepath data/tng_val/290-0.txt\n",
      "filepath data/tng_val/903-0.txt\n",
      "filepath data/tng_val/pg2097.txt\n",
      "filepath data/tng_val/356-0.txt\n",
      "filepath data/tng_val/48320-0.txt\n",
      "filepath data/tng_val/834-0.txt\n",
      "filepath data/tng_val/pg2344.txt\n"
     ]
    }
   ],
   "source": [
    "tng_messages = generate_ciphertexts('data/tng_val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features for clustering\n",
    "### Split ciphertexts into tokens\n",
    "### Get vectorized counts over whole corpus of ciphertexts, top 10 per message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_ngrams(texts, n):\n",
    "    for text_index in texts:\n",
    "        texts[text_index]['%s_grams' % str(n)] = [texts[text_index]['text'][k: k + n]\n",
    "                                                   for k in range(len(texts[text_index]['text']) - n)]\n",
    "        \n",
    "def top_N_in_doc(ngrams, top_N=vector_params['top_N']):\n",
    "    ngram_counts = Counter(ngrams).most_common(top_N)\n",
    "    return list([ngram for ngram in zip(*ngram_counts)][0])\n",
    "\n",
    "def populate_most_common_ngrams(texts, n, top_N=vector_params['top_N']):\n",
    "    for text_index in texts:\n",
    "        texts[text_index]['common_%s_grams' % str(n)] = top_N_in_doc(texts[text_index]['%s_grams' % str(n)])\n",
    "        \n",
    "def populate_n_grams_list(texts, n):\n",
    "    populate_ngrams(texts, n)\n",
    "    populate_most_common_ngrams(texts, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster messages by similarity\n",
    "## Group together any texts with at least 3 ngrams in common\n",
    "## Cluster ciphertexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_ngrams_in_common(ngram1, ngram2):\n",
    "    return len(set(ngram1).intersection(set(ngram2)))\n",
    "\n",
    "def populate_matching_messages(texts, ngrams='common_3_grams', min_matches=vector_params['min_matches']):\n",
    "    num_texts = len(texts.keys())\n",
    "    for i in range(num_texts):\n",
    "        messages_with_n_common = []\n",
    "        for j in range(num_texts):\n",
    "            num_in_common = num_ngrams_in_common(texts[i][ngrams], texts[j][ngrams])\n",
    "            if num_in_common > 0:\n",
    "                messages_with_n_common.append(j)\n",
    "        texts[i]['matched_messages'] = messages_with_n_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_array_of_matches(texts):\n",
    "    num_messages = len(texts.keys())\n",
    "    matches = np.zeros((num_messages, num_messages))\n",
    "    \n",
    "    for i in range(num_messages):\n",
    "        matches[i][i] = 1\n",
    "        for j in texts[i]['matched_messages']:\n",
    "            matches[i][j] = 1\n",
    "            \n",
    "    return matches\n",
    "\n",
    "\n",
    "def assign_clusters(matches, texts):\n",
    "    match_metric = DistanceMetric.get_metric('dice')\n",
    "    match_dist_mat = match_metric.pairwise(matches)\n",
    "    \n",
    "    clusters = AgglomerativeClustering(n_clusters=int(len(texts) / cipher_params['messages_per_source'])\n",
    "                                       , affinity=agg_params['affinity']\n",
    "                                       , linkage=agg_params['linkage'])\n",
    "    clusters.fit(match_dist_mat)\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        texts[i]['assigned_cluster'] = clusters.labels_[i]\n",
    "        \n",
    "        \n",
    "def get_cluster_assignments(texts, n_grams=decipher_params['n_grams']):\n",
    "    populate_n_grams_list(texts, n_grams)\n",
    "    populate_matching_messages(texts)\n",
    "    assign_clusters(make_array_of_matches(texts), texts)\n",
    "    \n",
    "    \n",
    "def combine_clusters(texts):\n",
    "    num_clusters = max([texts[i]['assigned_cluster'] for i in range(len(texts))]) + 1\n",
    "    clustered_texts = {i: {'text': ''} for i in range(num_clusters)}\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        cluster_num = texts[i]['assigned_cluster']\n",
    "        clustered_texts[cluster_num]['text'] = ''.join([clustered_texts[cluster_num]['text'], texts[i]['text']])\n",
    "        \n",
    "    return clustered_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_ciphertexts(messages):\n",
    "    get_cluster_assignments(messages, n_grams=decipher_params['n_grams'])\n",
    "    return combine_clusters(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tng_ciphers = cluster_ciphertexts(tng_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NBHJMFFPFZONHIKZCQXXFSRKFBYWLBWIUOUXIUTTCWKOMHIXQECXHQTLYHRZNYQXZNYGYQXLRLYMBHXSUCQTMMMHFLYXZFFBUVMMMGRCYYLUFYYHWQTYLHXDYLHXXFSVLYYPHOKOHLNGYZHXFOLHPMVIXQFBCVZMMYZXFMIQPMCXKBUNBLKWCNLPFCGHKAQNKXFQYWLAEIXORLCHKPFYVQDUXHFZNIRRDWIQCUXYQZQBYKXEBUGEUMFLQFFYVJUFYDQAOLHUBYHVBMHXSBDBUSPIYGDVPIUVJGWBEVTCGLCYSLHXPCHJLRNBLPBLIEIQGJULHYMWLNYWRODYWWVQMSHPUNBLKWCMHBTIQZBEBIXIPUJSOAUWKFFNBHPOINOXZXSDOPCHVMQWNROIUMVQUFFZOUNCQDUHNKBBULOLGLQKBZBIOJQMCQQQLLXMFYXKFYCOQAQLMWLAXNKXFSIXTQLYZOUNCQDMLYSLDNIIQTCMFXEYMDFPBYVLUUGGLZNSRRFBCQHUNGDVNYUOFFNFHMDYGDQGLYLZMHNKBXJNKFZECQDFBUWVAOLHSUXYQZQCMQLFWIPMXYNHIQMNUXPYEQBIGSIOUYHGQAIQHIXNIGFELYJXDXBLPIILGPTYFDFPXIZKTCMSBZUHGIAIEHAOOLLLGMFBXFBCPTTUNGLKIOPBMHGUEAFGHPAHFBQTUNWEQLYLPMHCPMALNDKFQCWKQMMZEAGSRRTUPHKANMHBZWUQVAOJULPOWHEUGCWEUHELZMHNKBZXIVLUQCOIPIGBYQMNKLIGUQVOIHVQMVFHPTUPHVAONKBDYUUBFBLHBICNKFZWUOIQRWHIXYHWPMCXKLXGYVJMSCDPWCZWEQSUUBMFFOXDAYDYXYVRAUYXPBZQCWEBIQHOROFYLUWYVFTUPHKAXIXYFNBHVMLYWEAOAKFRUCOQAMYHTTUNWEQCLYLUWYVEMPYWLPIQLQTCNSBDBUSPUWUQEQFJBLGNIVBQ'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tng_ciphers[0]['text'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break ciphertexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kasiski index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all repeated 4-grams, and their positions\n",
    "### Find most common gcd among all repeated 4-gram periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ngram_positions(texts, n=decipher_params['kasiski_n']):\n",
    "    for text_index in texts:\n",
    "        all_ngrams = {}\n",
    "        \n",
    "        for k in range(len(texts[text_index]['text']) - n):\n",
    "            ngram = texts[text_index]['text'][k: k + n]\n",
    "            if ngram not in all_ngrams.keys():\n",
    "                all_ngrams[ngram] = []\n",
    "            all_ngrams[ngram].append(k)\n",
    "        \n",
    "        texts[text_index]['%s_gram_positions' % str(n)] = {ngram: all_ngrams[ngram]\n",
    "                                                           for ngram in all_ngrams\n",
    "                                                           if len(all_ngrams[ngram]) >= decipher_params['min_repeats']}\n",
    "\n",
    "        \n",
    "def gen_ngram_periods(texts, n=decipher_params['kasiski_n']):\n",
    "    \n",
    "    period_key = '%s_gram_periods' % str(n)\n",
    "    position_key = '%s_gram_positions' % str(n)\n",
    "    \n",
    "    def get_periods(positions):\n",
    "        # print(ngram, len(positions), positions)\n",
    "        return [positions[i] - positions[i - 1]\n",
    "                for i in range(len(positions) - 1, 0, -1)\n",
    "                if positions[i] - positions[i - 1] < decipher_params['max_period']]\n",
    "    \n",
    "    for text_index in texts:\n",
    "        texts[text_index][period_key] = [get_periods(texts[text_index][position_key][ngram])\n",
    "                                            for ngram in texts[text_index][position_key]\n",
    "                                            if len(get_periods(texts[text_index][position_key][ngram])) > 0]\n",
    "        \n",
    "        texts[text_index][period_key] = set([item \n",
    "                                             for periods in texts[text_index][period_key]\n",
    "                                             for item in periods])\n",
    "        \n",
    "\n",
    "def gen_key_length(texts, n=decipher_params['kasiski_n']):\n",
    "\n",
    "    def gcd_list(nums):\n",
    "        return reduce(gcd, nums)\n",
    "    \n",
    "    def monte_carlo_key_len(periods):\n",
    "        # print(periods)\n",
    "        gcds = []\n",
    "        samples_per_trial = int(len(periods) / decipher_params['monte_carlo_sample_fraction'])\n",
    "        \n",
    "        for i in range(decipher_params['monte_carlo_num_trials']):\n",
    "            cand_gcd = gcd_list(np.random.choice(list(periods), samples_per_trial, replace=False))\n",
    "            if cand_gcd > 1:\n",
    "                gcds.append(cand_gcd)\n",
    "        # print(gcds)        \n",
    "        return Counter(gcds).most_common(1)[0][0]\n",
    "    \n",
    "    for text_index in texts:\n",
    "        try:\n",
    "            texts[text_index]['key_length'] = monte_carlo_key_len(texts[text_index]['%s_gram_periods' % str(n)])\n",
    "        except TypeError:\n",
    "            texts[text_index]['key_length'] = texts[text_index - 1]['key_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def est_kasiski_index(texts):\n",
    "    gen_ngram_positions(texts)\n",
    "    gen_ngram_periods(texts)\n",
    "    gen_key_length(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get frequency counts for each alphabet in key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_counts(ciphertext, key_len):\n",
    "    return [Counter(ciphertext[i::key_len]) for i in range(key_len)]\n",
    "\n",
    "def gen_freq_counts(texts):\n",
    "    for text_index in texts:\n",
    "        texts[text_index]['freq_counts'] = frequency_counts(texts[text_index]['text'], texts[text_index]['key_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_K_index_and_counts(texts):\n",
    "    est_kasiski_index(texts)\n",
    "    gen_freq_counts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guess that the key is the one that makes E the most common letter in the plaintext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rev_eng_key_letter(cand_key_letter, guess):\n",
    "    return letters[(letters.index(cand_key_letter) - letters.index(guess)) % 26]\n",
    "\n",
    "def rev_eng_key(cand_key):\n",
    "    guess = ''.join(['E' for _ in range(len(cand_key))])\n",
    "    return ''.join([rev_eng_key_letter(cand_key[i], guess[i]) for i in range(len(cand_key))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_keys(texts):\n",
    "    for text_index in texts:\n",
    "        texts[text_index]['key'] = rev_eng_key(''.join([texts[text_index]['freq_counts'][i].most_common(1)[0][0] for i in range(texts[text_index]['key_length'])]))\n",
    "\n",
    "def gen_plaintexts(texts):\n",
    "    for text_index in texts:\n",
    "        texts[text_index]['plaintext'] = translate_message(texts[text_index]['key'], texts[text_index]['text'], 'decrypt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrypt_all_ciphertexts(texts):\n",
    "    gen_K_index_and_counts(texts)\n",
    "    gen_keys(texts)\n",
    "    gen_plaintexts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TOOFORALLTHATHEWASSOSHORTMAHOMETSINGHWASLEFTTOGUARDTHEDOORWETOOKHIMTOAPLACEWHICHTHESIKHSHADALREADYPREPAREDITWASSOMEDISTANCEOFFWHEREAWINDINGPASSAGELEADSTOAGREATEMPTYHALLTHEBRICKWALLSOFWHICHWEREALLCRUMBLINGTOPIECESTHEEARTHFLOORHADSUNKINATONEPLACEMAKINGANATURALGRAVESOWELEFTACHMETTHEMERCHANTTHEREHAVINGFIRSTCOVEREDHIMOVERWITHLOOSEBRICKSTHISDONEWEALLWENTBACKTOTHETREASUREITLAYWHEREHEHADDROPPEDITWHENHEWASFIRSTATTACKEDTHEBOXWASTHESAMEWHICHNOWLIESOPENUPONYOURTABLEAKEYWASHUNGBYASILKENCORDTOTHATCARVEDHANDLEUPONTHETOPWEOPENEDITANDTHELIGHTOFTHELANTERNGLEAMEDUPONACOLLECTIONOFGEMSSUCHASIHAVEREADOFANDTHOUGHTABOUTWHENIWASALITTLELADATPERSHOREITWASBLINDINGTOLOOKUPONTHEMWHENWEHADFEASTEDOUREYESWETOOKTHEMALLOUTANDMADEALISTOFTHEMTHEREWEREONEHUNDREDANDFORTYTHREEDIAMONDSOFTHEFIRSTWATERINCLUDINGONEWHICHHASBEENCALLEDIBELIEVETHEGREATMOGULANDISSAIDTOBETHESECONDLARGESTSTONEINEXISTENCETHENTHEREWERENINETYSEVENVERYFINEEMERALDSANDONEHUNDREDANDSEVENTYRUBIESSOMEOFWHICHHOWEVERWERESMALLTHEREWEREFORTYCARBUNCLESTWOHUNDREDANDT'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decrypt_all_ciphertexts(tng_ciphers)\n",
    "tng_ciphers[12]['plaintext'][:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_shuffle_encrypt_cluster_decrypt(folderpath):\n",
    "    messages = generate_ciphertexts(folderpath)\n",
    "    ciphertexts = cluster_ciphertexts(messages)\n",
    "    decrypt_all_ciphertexts(ciphertexts)\n",
    "    return ciphertexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath data/test/pg456.txt\n",
      "filepath data/test/pg159.txt\n",
      "filepath data/test/pg35461.txt\n",
      "filepath data/test/pg1013.txt\n",
      "filepath data/test/pg5230.txt\n",
      "ROLLEDTHEINSECTINHIMWASCONFESSEDITSIMPLYILLUSTRATESTHEUNTHINKINGWAYINWHICHONEACQUIRESHABITSOFFEELING\n",
      "CROWNOFHISHEADLOOKOUTSAIDEVERYBODYFENCINGATRANDOMANDHITTINGATNOTHINGHOLDHIMSHUTTHEDOORDONTLETHIMLOOS\n",
      "WHICHHEHADLEFTMETHEBLACKWINDOWSTAREDATMELIKEANEYEATLASTWITHANEFFORTIPUTOUTTHELIGHTANDGOTINTOTHEHAMMO\n",
      "MIGHTSPEAKTOMEFORALITTLETIMEAPARTNOISAIDIHAVENOSECRETSFROMTHISLADYWHATDOYOUWANTTOTELLMEHESAIDITWASAT\n",
      "LANGUAGESRHINELANDRHINOCEROSRHODESRHODESIARHODESIANMANRICHELIEUCARDINALRICHMONDUSAROADSROBERTSONROBE\n"
     ]
    }
   ],
   "source": [
    "test_texts = split_shuffle_encrypt_cluster_decrypt('data/test')\n",
    "\n",
    "for text in test_texts:\n",
    "    print(test_texts[text]['plaintext'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine if plaintext has been translated into English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'T', 'A', 'O', 'I', 'N', 'S', 'R', 'H', 'L', 'D', 'C']\n"
     ]
    }
   ],
   "source": [
    "overall_monogram_freqs = pd.read_csv('data/english_monograms.txt', sep=' ', names=['count'])\n",
    "overall_monogram_freqs['freqs'] = overall_monogram_freqs['count'] / overall_monogram_freqs['count'].sum()\n",
    "overall_monogram_freqs.drop('count', axis=1, inplace=True)\n",
    "\n",
    "monogram_order = list(overall_monogram_freqs.index)[:12]\n",
    "print(monogram_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_plaintext_letter_order(texts):\n",
    "    for text_index in texts:\n",
    "        ordered_letters = Counter(texts[text_index]['plaintext']).most_common(26)\n",
    "        texts[text_index]['plain_freq_order'] = [ordered_letters[i][0] for i in range(12)]\n",
    "        \n",
    "def is_plaintext(text):\n",
    "    corr, _ = spearmanr(monogram_order, text['plain_freq_order'])\n",
    "    if corr > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_plaintext_letter_order(tng_ciphers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'T', 'O', 'A', 'I', 'N', 'S', 'H', 'R', 'L', 'D', 'U']\n"
     ]
    }
   ],
   "source": [
    "print(tng_ciphers[0]['plain_freq_order'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate recall over the Gutenberg dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath data/metrics/Robert Louis Stevenson___Prince Otto.txt\n",
      "filepath data/metrics/Edgar Rice Burroughs___Tarzan and the Jewels of Opar.txt\n",
      "filepath data/metrics/Anthony Trollope___Cousin Henry.txt\n",
      "filepath data/metrics/Jerome Klapka Jerome___They and I.txt\n",
      "filepath data/metrics/Anthony Trollope___The Fixed Period.txt\n",
      "filepath data/metrics/Ambrose Bierce___The Devil's Dictionary.txt\n",
      "filepath data/metrics/P G Wodehouse___Uneasy Money.txt\n",
      "filepath data/metrics/Hamlin Garland___Wayside Courtships.txt\n",
      "filepath data/metrics/P G Wodehouse___The Prince and Betty.txt\n",
      "filepath data/metrics/Edward Phillips Oppenheim___The Double Four.txt\n",
      "filepath data/metrics/Charles Kingsley___Twenty-Five Village Sermons.txt\n",
      "filepath data/metrics/William Makepeace Thackeray___Men's Wives.txt\n",
      "filepath data/metrics/Grant Allen___An African Millionaire.txt\n",
      "filepath data/metrics/Baronness Orczy___Castles in the Air.txt\n",
      "filepath data/metrics/Nathaniel Hawthorne___Passages from the American Notebooks, Volume 2.txt\n",
      "filepath data/metrics/Jacob Abbott___Cleopatra.txt\n",
      "filepath data/metrics/William Dean Howells___Suburban Sketches.txt\n",
      "filepath data/metrics/Grant Allen___Anglo-Saxon Britain.txt\n",
      "filepath data/metrics/P G Wodehouse___The Girl on the Boat.txt\n",
      "filepath data/metrics/Hamlin Garland___The Shadow World.txt\n",
      "filepath data/metrics/Robert Louis Stevenson___Records of a Family of Engineers.txt\n",
      "filepath data/metrics/D H Lawrence___England, My England.txt\n",
      "filepath data/metrics/Rudyard Kipling___Soldiers Three, Part 2.txt\n",
      "filepath data/metrics/Benjamin Franklin___Autobiography of Benjamin Franklin, Version 2.txt\n",
      "filepath data/metrics/Sir Arthur Conan Doyle___The Mystery of Cloomber.txt\n",
      "filepath data/metrics/Edward Stratemeyer___Out with Gun and Camera.txt\n",
      "filepath data/metrics/Rudyard Kipling___The Day's Work, Part 1.txt\n",
      "filepath data/metrics/Edward Phillips Oppenheim___Anna the Adventuress.txt\n",
      "filepath data/metrics/Daniel Defoe___The Consolidator.txt\n",
      "filepath data/metrics/Charlotte Mary Yonge___The Chosen People.txt\n",
      "filepath data/metrics/William Dean Howells___The Story of a Play.txt\n",
      "filepath data/metrics/George Bernard Shaw___Man And Superman.txt\n",
      "filepath data/metrics/John Morley___Burke.txt\n",
      "filepath data/metrics/Sir Arthur Conan Doyle___Round the Red Lamp.txt\n",
      "filepath data/metrics/Herbert George Wells___What is Coming?.txt\n",
      "filepath data/metrics/Jacob Abbott___Richard I.txt\n",
      "filepath data/metrics/James Joyce___Dubliners.txt\n",
      "filepath data/metrics/William Henry Hudson___Birds in Town and Village.txt\n",
      "filepath data/metrics/William Wymark Jacobs___Light Freights.txt\n",
      "filepath data/metrics/Hector Hugh Munro___The Toys of Peace.txt\n",
      "filepath data/metrics/Charles Kingsley___Sanitary and Social Lectures and Essays.txt\n",
      "filepath data/metrics/G K Chesterton___Orthodoxy.txt\n",
      "filepath data/metrics/Nathaniel Hawthorne___Tanglewood Tales.txt\n",
      "filepath data/metrics/Bret Harte___The Three Partners.txt\n",
      "filepath data/metrics/Jack London___Moon-Face and Other Stories.txt\n",
      "filepath data/metrics/William Dean Howells___A Traveler from Altruria.txt\n",
      "filepath data/metrics/Washington Irving___Wolfert's Roost and Miscellanies.txt\n",
      "filepath data/metrics/James Otis___The Search for the Silver City.txt\n",
      "filepath data/metrics/Lucy Maud Montgomery___Chronicles of Avonlea.txt\n",
      "filepath data/metrics/Edward Phillips Oppenheim___Jacob's Ladder.txt\n",
      "filepath data/metrics/Edward Phillips Oppenheim___The Devil's Paw.txt\n",
      "filepath data/metrics/Edward Phillips Oppenheim___The Zeppelin's Passenger.txt\n",
      "filepath data/metrics/Herbert George Wells___The World Set Free.txt\n",
      "filepath data/metrics/Thomas Henry Huxley___Hume.txt\n",
      "filepath data/metrics/Anthony Trollope___Thackeray.txt\n",
      "filepath data/metrics/Howard Pyle___Men of Iron.txt\n",
      "filepath data/metrics/Charlotte Mary Yonge___Two Penniless Princesses.txt\n",
      "filepath data/metrics/Wilkie Collins___A Rogue's Life.txt\n",
      "filepath data/metrics/Stephen Leacock___Arcadian Adventures with the Idle Rich.txt\n",
      "filepath data/metrics/John Ruskin___Proserpina, Volume 1.txt\n",
      "filepath data/metrics/Charlotte Mary Yonge___Friarswood Post-Office.txt\n",
      "filepath data/metrics/Edward Stratemeyer___Dave Porter At Bear Camp.txt\n",
      "filepath data/metrics/George Bernard Shaw___Heartbreak House.txt\n",
      "filepath data/metrics/Louisa May Alcott___A Garland for Girls.txt\n",
      "filepath data/metrics/Charlotte Mary Yonge___Young Folks' History of Rome.txt\n",
      "filepath data/metrics/Thomas Carlyle___History of Friedrich II of Prussia, Volume 13.txt\n",
      "filepath data/metrics/P G Wodehouse___The Intrusion of Jimmy.txt\n",
      "filepath data/metrics/Edgar Rice Burroughs___The Outlaw of Torn.txt\n",
      "filepath data/metrics/Edward Stratemeyer___Fighting in Cuban Waters.txt\n",
      "filepath data/metrics/Edgar Allan Poe___Edgar Allan Poe's Complete Poetical Works.txt\n",
      "filepath data/metrics/Sir Arthur Conan Doyle___A Duet.txt\n",
      "filepath data/metrics/Edward Phillips Oppenheim___The Double Life Of Mr. Alfred Burton.txt\n",
      "filepath data/metrics/Lyman Frank Baum___Tik-Tok of Oz.txt\n",
      "filepath data/metrics/John Ruskin___Sesame and Lilies.txt\n",
      "filepath data/metrics/Charles Kingsley___The Water-Babies.txt\n",
      "filepath data/metrics/Anthony Trollope___Linda Tressel.txt\n",
      "filepath data/metrics/Herbert George Wells___The Secret Places of the Heart.txt\n",
      "filepath data/metrics/Henry Rider Haggard___Mr. Meeson's Will.txt\n",
      "filepath data/metrics/R M Ballantyne___Philosopher Jack.txt\n",
      "filepath data/metrics/Mary Stewart Daggett___Mariposilla.txt\n",
      "filepath data/metrics/Lyman Frank Baum___Rinkitink in Oz.txt\n",
      "filepath data/metrics/Lyman Frank Baum___Aunt Jane's Nieces at Work.txt\n",
      "filepath data/metrics/Bret Harte___The Bell-Ringer of Angel's and Other Stories.txt\n",
      "filepath data/metrics/Henry James___Washington Square.txt\n",
      "filepath data/metrics/Edward Phillips Oppenheim___The Great Prince Shan.txt\n",
      "filepath data/metrics/Edgar Rice Burroughs___A Princess of Mars.txt\n",
      "filepath data/metrics/Henry Rider Haggard___Allan's Wife.txt\n",
      "filepath data/metrics/Nathaniel Hawthorne___The Marble Faun, Volume 1.txt\n",
      "filepath data/metrics/Jerome Klapka Jerome___Novel Notes.txt\n",
      "filepath data/metrics/William Wymark Jacobs___A Master of Craft.txt\n",
      "filepath data/metrics/Sir Arthur Conan Doyle___The Great Keinplatz Experiment and Other Tales of Twilight and the Unseen.txt\n",
      "filepath data/metrics/William Makepeace Thackeray___Notes on a Journey from Cornhill to Grand Cairo.txt\n",
      "filepath data/metrics/Bret Harte___Under the Redwoods.txt\n",
      "filepath data/metrics/Hector Hugh Munro___Beasts and Super-Beasts.txt\n",
      "filepath data/metrics/William Makepeace Thackeray___Catherine: A Story.txt\n",
      "filepath data/metrics/Thomas Hardy___The Well-Beloved.txt\n",
      "filepath data/metrics/Edward Stratemeyer___The Rover Boys on a Tour.txt\n",
      "filepath data/metrics/Howard Pyle___Stolen Treasure.txt\n",
      "filepath data/metrics/Jack London___John Barleycorn.txt\n",
      "filepath data/metrics/D H Lawrence___Fantasia of the Unconscious.txt\n",
      "filepath data/metrics/William Dean Howells___The Daughter of the Storage.txt\n",
      "filepath data/metrics/Oscar Wilde___A Critic in Pall Mall.txt\n",
      "filepath data/metrics/William Wymark Jacobs___More Cargoes.txt\n",
      "filepath data/metrics/Wilkie Collins___Rambles Beyond Railways.txt\n",
      "filepath data/metrics/Edgar Rice Burroughs___Thuvia, Maid of Mars.txt\n",
      "filepath data/metrics/Thornton Waldo Burgess___The Boy Scouts in A Trapper's Camp.txt\n",
      "filepath data/metrics/Henry James___The Spoils of Poynton.txt\n",
      "filepath data/metrics/Herbert George Wells___Love and Mr. Lewisham.txt\n",
      "filepath data/metrics/Charlotte Mary Yonge___The Prince and the Page.txt\n",
      "filepath data/metrics/Bret Harte___Complete Poetical Works of Bret Harte.txt\n",
      "filepath data/metrics/Bret Harte___Tales of Trail and Town.txt\n",
      "filepath data/metrics/Sir Arthur Conan Doyle___The Dealings of Captain Sharkey and Other Tales of Pirates.txt\n",
      "filepath data/metrics/Howard Pyle___Twilight Land.txt\n",
      "filepath data/metrics/Jacob Abbott___Richard II.txt\n",
      "filepath data/metrics/Rudyard Kipling___Actions and Reactions.txt\n",
      "filepath data/metrics/Sir Francis Galton___Finger Prints.txt\n",
      "filepath data/metrics/Robert Louis Stevenson___A Footnote to History.txt\n",
      "filepath data/metrics/Herbert George Wells___The First Men In The Moon.txt\n",
      "filepath data/metrics/William Makepeace Thackeray___The Book of Snobs.txt\n",
      "filepath data/metrics/Anthony Trollope___An Eye for an Eye.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath data/metrics/Hamlin Garland___Victor Ollnee's Discipline.txt\n",
      "filepath data/metrics/Frank Richard Stockton___A Bicycle of Cathay.txt\n",
      "filepath data/metrics/Edward Phillips Oppenheim___The Survivor.txt\n",
      "filepath data/metrics/Anthony Trollope___The Struggles of Brown, Jones, and Robinson.txt\n",
      "filepath data/metrics/William Henry Hudson___Birds and Man.txt\n",
      "filepath data/metrics/James Otis___A Runaway Brig.txt\n",
      "filepath data/metrics/George Alfred Henty___Among Malay Pirates.txt\n",
      "filepath data/metrics/G K Chesterton___Heretics.txt\n",
      "filepath data/metrics/Anthony Trollope___The Golden Lion of Granpere.txt\n",
      "filepath data/metrics/Edward Phillips Oppenheim___The Governors.txt\n",
      "filepath data/metrics/Frank Richard Stockton___The Associate Hermits.txt\n",
      "filepath data/metrics/Edward Stratemeyer___The Rover Boys in Camp.txt\n",
      "filepath data/metrics/Baronness Orczy___I Will Repay.txt\n",
      "filepath data/metrics/William Dean Howells___Through the Eye of the Needle.txt\n",
      "filepath data/metrics/Anthony Trollope___Sir Harry Hotspur of Humblethwaite.txt\n",
      "filepath data/metrics/P G Wodehouse___The Man with Two Left Feet.txt\n",
      "filepath data/metrics/Mark Twain___The American Claimant.txt\n",
      "filepath data/metrics/O Henry___Cabbages and Kings.txt\n",
      "filepath data/metrics/John Ruskin___Our Fathers Have Told Us, Part I, The Bible of Amiens.txt\n",
      "filepath data/metrics/George Alfred Henty___Sturdy and Strong.txt\n",
      "filepath data/metrics/James Otis___Down the Slope.txt\n",
      "filepath data/metrics/Rudyard Kipling___Stalky & Co.txt\n",
      "filepath data/metrics/Harold Bindloss___The Intriguers.txt\n",
      "filepath data/metrics/Herman Melville___Israel Potter.txt\n",
      "filepath data/metrics/Robert Louis Stevenson___Treasure Island.txt\n",
      "filepath data/metrics/Jacob Abbott___Genghis Khan.txt\n",
      "filepath data/metrics/Jerome Klapka Jerome___Three Men in a Boat.txt\n",
      "filepath data/metrics/Sir Arthur Conan Doyle___The Man from Archangel.txt\n",
      "filepath data/metrics/Thomas Crofton Croker___A Walk from London to Fulham.txt\n"
     ]
    }
   ],
   "source": [
    "gutenberg = split_shuffle_encrypt_cluster_decrypt('data/metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "0.7583892617449665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bam/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:248: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.\n",
      "  \"values. nan values will be ignored.\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "gen_plaintext_letter_order(gutenberg)\n",
    "\n",
    "english_plaintext_count = 0\n",
    "for text in gutenberg:\n",
    "    if is_plaintext(gutenberg[text]):\n",
    "        english_plaintext_count += 1\n",
    "        \n",
    "print(english_plaintext_count)\n",
    "print(english_plaintext_count/ len(gutenberg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
